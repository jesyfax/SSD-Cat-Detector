{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "## Datasets\n",
    "\n",
    "The current version only supports Pascal VOC datasets (2007 and 2012). In order to be used for training a SSD model, the former need to be converted to TF-Records using the `tf_convert_data.py` script:\n",
    "```bash\n",
    "DATASET_DIR=./VOC2007/test/\n",
    "OUTPUT_DIR=./tfrecords\n",
    "python tf_convert_data.py \\\n",
    "    --dataset_name=pascalvoc \\\n",
    "    --dataset_dir=${DATASET_DIR} \\\n",
    "    --output_name=voc_2007_train \\\n",
    "    --output_dir=${OUTPUT_DIR}\n",
    "```\n",
    "Note the previous command generated a collection of TF-Records instead of a single file in order to ease shuffling during training.\n",
    "\n",
    "## Evaluation on Pascal VOC 2007\n",
    "\n",
    "The present TensorFlow implementation of SSD models have the following performances:\n",
    "\n",
    "| Model | Training data  | Testing data | mAP | FPS  |\n",
    "|--------|:---------:|:------:|:------:|:------:|\n",
    "| [SSD-300 VGG-based](https://drive.google.com/open?id=0B0qPCUZ-3YwWZlJaRTRRQWRFYXM) | VOC07+12 trainval | VOC07 test | 0.778 | - |\n",
    "| [SSD-300 VGG-based](https://drive.google.com/file/d/0B0qPCUZ-3YwWUXh4UHJrd1RDM3c/view?usp=sharing) | VOC07+12+COCO trainval | VOC07 test | 0.817 | - |\n",
    "| [SSD-512 VGG-based](https://drive.google.com/open?id=0B0qPCUZ-3YwWT1RCLVZNN3RTVEU) | VOC07+12+COCO trainval | VOC07 test | 0.837 | - |\n",
    "\n",
    "\n",
    "\n",
    "After downloading and extracting the previous checkpoints, the evaluation metrics should be reproducible by running the following command:\n",
    "```bash\n",
    "EVAL_DIR=./logs/\n",
    "CHECKPOINT_PATH=./checkpoints/VGG_VOC0712_SSD_300x300_ft_iter_120000.ckpt\n",
    "python eval_ssd_network.py \\\n",
    "    --eval_dir=${EVAL_DIR} \\\n",
    "    --dataset_dir=${DATASET_DIR} \\\n",
    "    --dataset_name=pascalvoc_2007 \\\n",
    "    --dataset_split_name=test \\\n",
    "    --model_name=ssd_512_vgg \\\n",
    "    --checkpoint_path=${CHECKPOINT_PATH} \\\n",
    "    --batch_size=1\n",
    "```\n",
    "The evaluation script provides estimates on the recall-precision curve and compute the mAP metrics following the Pascal VOC 2007 and 2012 guidelines.\n",
    "\n",
    "In addition, if one wants to experiment/test a different Caffe SSD checkpoint, the former can be converted to TensorFlow checkpoints as following:\n",
    "\n",
    "\n",
    "## Training\n",
    "\n",
    "The script `train_ssd_network.py` is in charge of training the network. Similarly to TF-Slim models, one can pass numerous options to the training process (dataset, optimiser, hyper-parameters, model, ...). In particular, it is possible to provide a checkpoint file which can be use as starting point in order to fine-tune a network.\n",
    "\n",
    "### Fine-tuning existing SSD checkpoints\n",
    "\n",
    "The easiest way to fine the SSD model is to use as pre-trained SSD network (VGG-300 or VGG-512). For instance, one can fine a model starting from the former as following:\n",
    "```bash\n",
    "DATASET_DIR=./tfrecords\n",
    "TRAIN_DIR=./logs/\n",
    "CHECKPOINT_PATH=./checkpoints/ssd_300_vgg.ckpt\n",
    "python train_ssd_network.py \\\n",
    "    --train_dir=${TRAIN_DIR} \\\n",
    "    --dataset_dir=${DATASET_DIR} \\\n",
    "    --dataset_name=pascalvoc_2012 \\\n",
    "    --dataset_split_name=train \\\n",
    "    --model_name=ssd_512_vgg \\\n",
    "    --checkpoint_path=${CHECKPOINT_PATH} \\\n",
    "    --save_summaries_secs=60 \\\n",
    "    --save_interval_secs=600 \\\n",
    "    --weight_decay=0.0005 \\\n",
    "    --optimizer=adam \\\n",
    "    --learning_rate=0.001 \\\n",
    "    --batch_size=32\n",
    "```\n",
    "Note that in addition to the training script flags, one may also want to experiment with data augmentation parameters (random cropping, resolution, ...) in `ssd_vgg_preprocessing.py` or/and network parameters (feature layers, anchors boxes, ...) in `ssd_vgg_300/512.py`\n",
    "\n",
    "Furthermore, the training script can be combined with the evaluation routine in order to monitor the performance of saved checkpoints on a validation dataset. For that purpose, one can pass to training and validation scripts a GPU memory upper limit such that both can run in parallel on the same device. If some GPU memory is available for the evaluation script, the former can be run in parallel as follows:\n",
    "```bash\n",
    "EVAL_DIR=${TRAIN_DIR}/eval\n",
    "python eval_ssd_network.py \\\n",
    "    --eval_dir=${EVAL_DIR} \\\n",
    "    --dataset_dir=${DATASET_DIR} \\\n",
    "    --dataset_name=pascalvoc_2007 \\\n",
    "    --dataset_split_name=test \\\n",
    "    --model_name=ssd_300_vgg \\\n",
    "    --checkpoint_path=${TRAIN_DIR} \\\n",
    "    --wait_for_checkpoints=True \\\n",
    "    --batch_size=1 \\\n",
    "    --max_num_batches=500\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
